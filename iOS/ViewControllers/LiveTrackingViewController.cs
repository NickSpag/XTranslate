// This file has been autogenerated from a class added in the UI designer.

using System;
using System.Threading.Tasks;
using AVFoundation;
using CoreAnimation;
using CoreFoundation;
using CoreGraphics;
using CoreMedia;
using CoreVideo;
using Foundation;
using ImageIO;
using UIKit;
using Vision;
using System.Linq;

namespace XTranslate.iOS
{
    public partial class LiveTrackingViewController : UIViewController, IAVCaptureVideoDataOutputSampleBufferDelegate
    {
        #region Fields: Private
        private AVCaptureSession captureSession;
        private AVCaptureDeviceInput deviceInput;
        private AVCaptureVideoDataOutput deviceOutput;
        private AVCaptureDevice captureDevice;

        private DispatchQueue queue = new DispatchQueue("videoQueue");

        private CVPixelBufferAttributes pixelBufferAttributes = new CVPixelBufferAttributes()
        {
            PixelFormatType = CVPixelFormatType.CV32BGRA
        };

        private NSMutableDictionary videoSettings;
        #endregion
        private VNRequest[] requests;


        public LiveTrackingViewController(IntPtr handle) : base(handle)
        {
        }

        public override void ViewDidLoad()
        {
            base.ViewDidLoad();

            //AuthorizeCameraUse();
            StartLiveVideo();
            StartTextDetection();
        }

        public override void DidReceiveMemoryWarning()
        {
            base.DidReceiveMemoryWarning();

            System.Console.WriteLine("memory warning");
        }

        private void StartLiveVideo()
        {
            captureSession = new AVCaptureSession();
            captureSession.SessionPreset = AVCaptureSession.PresetHigh;

            captureDevice = AVCaptureDevice.GetDefaultDevice(AVMediaTypes.Video);

            deviceInput = AVCaptureDeviceInput.FromDevice(captureDevice);
            deviceOutput = new AVCaptureVideoDataOutput();

            var settings = new CVPixelBufferAttributes
            {
                PixelFormatType = CVPixelFormatType.CV32BGRA
            };
            deviceOutput.WeakVideoSettings = settings.Dictionary;
            //deviceOutput.AlwaysDiscardsLateVideoFrames = true;

            //var capVideo = new AVCaptureVideoDataOutput();
            //deviceOutput.Connections[0].CameraIntrinsicMatrixDeliveryEnabled = true;

            //videoSettings = new NSMutableDictionary();
            //videoSettings[CVPixelBuffer.PixelFormatTypeKey] = new NSNumber((int)CVPixelFormatType.CV32BGRA);

            //deviceOutput.WeakVideoSettings = videoSettings;
            deviceOutput.SetSampleBufferDelegateQueue(this, queue);

            try
            {
                captureSession.AddInput(deviceInput);
                captureSession.AddOutput(deviceOutput);
            }
            catch (Exception ex)
            {
                System.Console.WriteLine(ex.Message);
            }

            ConfigureImageLayer(captureSession);

            captureSession.StartRunning();
        }

        private void ConfigureImageLayer(AVCaptureSession session)
        {
            var imageLayer = new AVCaptureVideoPreviewLayer(session);
            imageLayer.VideoGravity = AVLayerVideoGravity.ResizeAspectFill;
            imageLayer.Frame = cameraView.Bounds;
            cameraView.Layer.AddSublayer(imageLayer);
        }

        private void StartTextDetection()
        {
            var textRequest = new VNDetectTextRectanglesRequest(VNRequestCompletionHandler);
            textRequest.ReportCharacterBoxes = true;

            requests = new VNRequest[] { textRequest };

        }

        private void VNRequestCompletionHandler(VNRequest request, NSError error)
        {
            //System.Console.WriteLine(request);
            var observations = request.GetResults<VNTextObservation>();

            //var results = observations.
            DispatchQueue.DefaultGlobalQueue.DispatchAsync(() =>
            {
                foreach (var observation in observations)
                {
                    //Task.Run(() => HighlightWord(observation)); //
                    HighlightWord(observation);
                }

            });

        }

        private void HighlightWord(VNTextObservation textObservation)
        {
            nfloat maxX = 9999;
            nfloat minX = 0;
            nfloat maxY = 9999;
            nfloat minY = 0;

            foreach (var box in textObservation.CharacterBoxes)
            {
                if (box.BottomLeft.X is var bottomLeft && bottomLeft < maxX)
                    maxX = bottomLeft;

                if (box.BottomRight.X is var bottomRight && bottomRight > maxX)
                    minX = bottomRight;

                if (box.BottomRight.Y is var bottomRightY && bottomRightY < maxY)
                    maxY = bottomRightY;

                if (box.TopRight.Y is var topRightY && topRightY > minY)
                    minY = topRightY;
            }

            var xCord = maxX * cameraView.Frame.Size.Width;
            var yCord = (1 - minY) * cameraView.Frame.Size.Height;
            var width = (minX - maxX) * cameraView.Frame.Size.Width;
            var height = (minY - maxY) * cameraView.Frame.Size.Height;

            var outline = new CALayer();
            outline.Frame = new CGRect(xCord, yCord, width, height);
            outline.BorderWidth = 2;
            outline.BorderColor = new CGColor(255, 0, 0);


            if (cameraView.Layer.Sublayers.Count() > 1)
            {
                cameraView.Layer.Sublayers[1].RemoveFromSuperLayer();
            }

            cameraView.Layer.AddSublayer(outline);

            System.Console.WriteLine("hightlight added");
        }

        #region Methods: IAVCaptureVideoDataOutputSampleBufferDelegate
        //used in StartLiveVideo, SetSampleBufferDelegateQueue
        [Export("captureOutput:didOutputSampleBuffer:fromConnection:")]
        public void DidOutputSampleBuffer(AVCaptureOutput captureOutput, CMSampleBuffer sampleBuffer, AVCaptureConnection connection)
        {
            var imageBuffer = sampleBuffer.GetImageBuffer();
            var camerMatrixAttachment = sampleBuffer.GetSampleAttachments(true)[0];

            sampleBuffer.Dispose();

            var pixelBuffer = imageBuffer as CVPixelBuffer;

            var requestOptions = new VNImageOptions();
            var matrix = camerMatrixAttachment.CameraIntrinsicMatrix;
            requestOptions.CameraIntrinsics = matrix;

            var imageRequestHandler = new VNImageRequestHandler(pixelBuffer, CGImagePropertyOrientation.Right, requestOptions);

            if (requests != null)
                imageRequestHandler.Perform(requests, out NSError error);

            System.Console.WriteLine("finished");
        }

        [Export("captureOutput:didDropSampleBuffer:fromConnection:")]
        public void DidDropSampleBuffer(AVCaptureOutput captureOutput, CMSampleBuffer sampleBuffer, AVCaptureConnection connection)
        {
            System.Console.WriteLine("dropped frame");
            sampleBuffer.Dispose();
        }
        #endregion
    }
}
